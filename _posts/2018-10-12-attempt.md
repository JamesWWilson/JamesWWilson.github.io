---
title: "Stats 102C, Homework 1"
output: html_document
author: James Wilson
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Homework Questions, copyright Miles Chen. Do not post or distribute without permission.

**Do not post your solutions online on a site like github. Violations will be reported to the Dean of Students.**

## Reading and Viewing:

- Introducing Monte Carlo Methods with R: Section 2.1, and Section 2.3
- Kolmogorov-Smirnov Test on Youtube: <https://www.youtube.com/watch?v=ZO2RmSkXK3c> (This video covers the two-sample test, but we will conduct a one-sample test against a reference distribution)

## Problem 1 - Estimate pi (poorly)

A "fun" Monte Carlo Exercise ... Get a bad estimate of pi by using random uniform numbers.

In this first exercise, we can see how a simple source of randomness (in our case, R's `runif()` function) can be used to estimate tough quantities.

We will find an estimate of pi by estimating the ratio between the area of a circle and its encompassing square.

```{r}
s <- seq(-1,1, by = 0.001)
posf <- sqrt(1-s^2)
plot(s, posf, type = "l", asp = 1, ylim = c(-1,1))
lines(s, -1*posf)
segments(-1,-1,-1,1)
segments(-1,-1,1,-1)
segments(1,1,-1,1)
segments(1,1,1,-1)
```

To calculate the area of the circle analytically, we would need to integrate the function drawing the upper semi-circle and then multiply that by 2. This process requires the use of trig substitutions, and while doable, can illustrate a time where the analytic solution is not easy.

$$Area = 2 \times \int_{-1}^1 \sqrt{1 - x^2} dx$$

For the Monte-Carlo approach, we will use `runif(n, min = -1, max=1)` to generate a bunch of random pairs of x and y coordinates. We will see how many of those random uniform points fall within the circle. This is easy - just see if $x^2 + y^2 \le 1$. The total area of the square is 4. The total area of the circle is pi. Thus, the proportion of coordinates that satisfy the inequality  $x^2 + y^2 \le 1 \approx \pi/4$.

Instructions:

- create a vector x of n random values between -1 and 1. I suggest starting with n = 500
- create a vector y of n random values between -1 and 1. Use the two vectors to make coordinate pairs.
- calculate which of points satisfy the inequality for falling inside the circle.
- Print out your estimate of pi by multiplying the proportion by 4.
- plot each of those (x,y) coordinate pairs. Use pch = 20. Color the points based on whether they fall in the circle or not.

```{r}
set.seed(3)
x <- runif(500,min = -1, max = 1)
y <- runif(500,min = -1, max = 1)

inside <- (x^2 + y^2 <= 1)
outside <- (x^2 + y^2 > 1)

table <- table(inside)
prop <- (table[2][1]/500)
pi.estimate <- prop*4
# Our estimate for pi is...
pi.estimate

# create the plot with points colored according to whether they fall in or out of the circle
s <- seq(-1,1, by = 0.001)
posf <- sqrt(1-s^2)
plot(s, posf, type = "l", asp = 1, ylim = c(-1,1))
lines(s, -1*posf)
segments(-1,-1,-1,1)
segments(-1,-1,1,-1)
segments(1,1,-1,1)
segments(1,1,1,-1)
points(x[inside],y[inside],cex=1, pch=20, col="green")
points(x[outside],y[outside],cex=1, pch=20, col="red")

```


## Problem 2

Write a function `my_rexp(n, rate)`, that will generate `n` random values drawn from an exponential distribution with lambda = "rate" by using the inverse CDF method. Use `runif()` as your sole source of randomness.

You are not allowed to use any of the functions `dexp()`, `pexp()`, `qexp()`, or `rexp()`. 

Use your function to generate 500 random samples from an exponential distribution with lambda  = 1.

After generating 500 samples, plot the empirical CDF function of your data (see `ecdf`). Add the theoretic CDF of the exponential distribution to the same plot (in a different color). You can use R's pexp() function to add the theoretic CDF. 

Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic exponential distribution. Be sure to print out the resulting p-value and comment on the sample produced by your function.

```{r}
set.seed(2)
my_rexp <- function(n, rate){
  u <- runif(n, min = 0, max = 1)
  x <- -(log(u)/rate) #formula, inverse CDF 
}
new_exp <- my_rexp(500,1)

# Plot of empirical CDF data 
plot(ecdf(new_exp), main = "Rexp Distribution", do.points = TRUE)
vals <- seq(0.01, max(new_exp), by = 0.01)
lines(vals, pexp(vals, rate = 1), col = "red")

# KS Test of Significance
ks.test(new_exp, pexp)
pval <- ks.test(new_exp, pexp)[[2]]
pval
```

The given p-value means we will reject the alternative hypothesis, as there is not enough evidence to accept it. We accept that the two samples were drawn from the same distribution. the D statistics is also small, which re-enforces this evidence.


## Problem 3

Write a function `my_rbinom(n, size, prob)`, that will generate `n` random values drawn from a binomial distribution with size = `size` and probability of success = `prob` by using the inverse CDF method. Use `runif()` as your sole source of randomness.

Do not use any of R's binom functions. Do not use `dbinom`, `pbinom`, `qbinom()`, or `rbinom()`

Use your function `my_rbinom()` to generate 200 values from a binomial distribution with n = 6, and p = 0.4.

After generating 200 samples, make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution.

Use a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test.

```{r}
set.seed(1)
# Create my_rbinom function
my_rbinom <- function(n, size, prob){
  list_of_xvals <- 1:size
  list_of_probs <- c()
  d <- c()
  for(i in 0:size){
    # mock binomial distribution 
    list_of_probs[i+1] <- (factorial(size)/(factorial(size-i)*factorial(i)))*(prob^i)*((1-prob)^(size-i))
    d[i+1] <- sum(list_of_probs)
  }
  list_u <- runif(n)
  interval <- c(0, d)
  results <- c()
  # create interval
  for(j in 1:length(list_u)){
    for(i in 2:length(interval)){
    if((list_u[j] > interval[i-1]) & (list_u[j] < interval[i])){
      results[j] <- (0:size)[i-1]}
    }
  }
print(results)
}

# Generate 200 samples 
my_samp <- my_rbinom(200, 6, 0.4)

#Empirical 
x <- prop.table(table(my_samp))
x <- as.vector(x)

#Theoretical 
p <- dbinom(0:6, 6, .4)
p[6] <- p[6]+p[7]
p <- p[1:6]

#Side-by-side barplot
data_table <- cbind(x,p)
data <- c(data_table[1,1],data_table[1,2],data_table[2,1],data_table[2,2],
          data_table[3,1],data_table[3,2],data_table[4,1],data_table[4,2],
          data_table[5,1],data_table[5,2],data_table[6,1],data_table[6,2])
barplot(matrix(data,nr=2), 
        beside=T, col=c("blue","red"), 
        names.arg=c(0:5),  ylab="Proportion", xlab="Size", 
        main="Binomial PMFs", ylim=c(0,0.4))
legend("topright", c("Empirical PMF","Theoretical PMF"), pch=15, col=c("blue","red"), bty="n")


#Chi-Square Test
obs <- rep(NA, 6)
for(i in 1:6){
  obs[i] <- table(x)[[i]]
} 
exp_true <- 200*p #200 trials

chisq_data <- as.data.frame(cbind(obs,p,exp_true))
chisq_data
chisq.test(chisq_data[,c(1,3)])
pval <- chisq.test(chisq_data[,c(1,3)])[[3]]
pval 

```

The p-value of indicates that we fail to reject the alternative hypothesis. It is clear that the the list of all possible categories and their expected proportions are relatively similar. We do not have enough evidence to suggest that at least one of the proportions is wrong, and the plot supports this suggestion.


## Problem 4

Let $f(x)$ and $g(x)$ be the target and candidate (proposal) distributions, respectively, in acceptance-rejection sampling. Find the optimal constant M that maximizes the acceptance rates for the following designs.

$f(x) = \frac{1}{2} \sin(x)$ for $0 \le x \le \pi$

$g(x) = \mbox{Unif}(0, \pi)$

Answer: M is $\pi/2$

Implement the rejection sampling design, using `runif(n, 0, pi)` as your source of randomness. Generate 500 samples.


```{r}
set.seed(45)
proposed <-runif(500, 0, pi)
f <-function(x) 0.5 * sin(x)
g <-function(x) 1/pi
M <-pi/2
u <-runif(500, 0, 1)

acceptance <-(u <= f(proposed)/(M*g(proposed)))
samp <-proposed[acceptance]
acceptance <- data.frame(acceptance) # change to data frame 
tru_vals <- length(acceptance[acceptance == TRUE])

#The acceptance rate will be...
rate <- tru_vals/500
rate
```

```{r}

# Create a histogram of your generated (accepted) sample.
hist(samp, main = "Acceptance Sample Histogram", col = "darkgreen")

# Plot a kernel density of the resulting (accepted) sample.
plot(density(samp), main = "Acceptance Sample Density", col = "darkgreen")

```




## Problem 5

Use rejection sampling to generate samples from the normal distribution, by using the folded-normal distribution method discussed in class.

The standard normal distribution has the pdf:

$$f(z) = \frac{1}{\sqrt{2\pi}} \exp{(-z^2/2)} \mbox{,   for } z \in (-\infty, \infty)$$

The target distribution f(x) will be the positive half of the standard normal distribution, which will have PDF:

$$f(x) = 2 \times \frac{1}{\sqrt{2\pi}} \exp{(-x^2/2)}\mbox{,   for } x \ge 0$$

Use an exponential distribution with lambda = 1 as your trial (proposal) distribution.

$$g(x) = e^{-x} \mbox{,   for } x \ge 0$$

Find the optimal constant M that maximizes the acceptance rates for the rejection sampling design.

Implement the rejection sampling design as discussed in class.

- Use `runif` and inverse CDF to get a proposal value $X$ from the exponential distribution.
- Calculate the ratio: $\frac{f(X)}{M \times g(X)}$
- Use `runif` to generate $U$ to decide whether to accept or reject the proposed $X$.
- keep the accepted $X$
- Use `runif` to generate $S$ to decide whether the accepted $X$ will be positive or negative with probably 0.5.

```{r}
set.seed(10)

proposed <-rexp(200, rate = 1)
f <-function(x) 2/sqrt(2 * pi) * exp(-x^2/2)
g <-function(x) exp(-x)
s <- seq(-.05, 3.5, by=0.001)
M <- 1.32 
u <-runif(200)

acceptance <-(u <= f(proposed)/(M*g(proposed)) )
samp <-proposed[acceptance]
acceptance <- data.frame(acceptance) # change to data frame 
tru_vals <- length(acceptance[acceptance == TRUE])

#The acceptance rate will be...
rate <- tru_vals/200
rate #0.7399

#Create a histogram of your generated sample.
hist(samp, breaks = 30, col = "darkblue", main = "Historgram of Sample")
plot(density(samp), col = "darkblue", main = "Density Plot of Sample")

#Create a QQ-norm plot.
qqnorm(samp, col = "darkblue", main = "QQNorm of Sample")

```






